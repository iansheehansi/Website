{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjR9TqGqMROM"
      },
      "source": [
        "# Text2Light\n",
        "This notebook is an all-in-one inference script to generate HDR panorama from free-form texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vi172hqMROP"
      },
      "source": [
        "# Setup Environment\n",
        "This section is intended for run the script in a Colab environment. For a full local setup, we recommend to use [conda environment](https://github.com/FrozenBurning/Text2Light#Installation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dToOdsFhMhMd",
        "outputId": "5f31d5a7-7f05-4d01-e445-4f474e997c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mPr3BUF0QWPQ",
        "outputId": "a359a201-c4da-47a5-ad94-dcf871c610f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Text2Light' already exists and is not an empty directory.\n",
            "/content/Text2Light/Text2Light\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/FrozenBurning/Text2Light\n",
        "%cd Text2Light\n",
        "!mkdir -p logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoJwamQ_MROP",
        "outputId": "d78ebe13-0041-4d22-edf7-497771fae821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (1:14.0-55~exp2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.5.1.post0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.7.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.5.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.14.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.14.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (2.2.6)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (6.31.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (1.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.12.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.6.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from kornia) (0.1.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch>=1.9.1->kornia) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.0.1)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.17.61, 4.4.0.42, 4.4.0.44, 4.5.4.58, 4.5.5.62, 4.7.0.68\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.1.2.30 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84, 4.11.0.86)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.1.2.30\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install libomp-dev\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning tensorboardX einops transformers\n",
        "!pip install kornia\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install faiss\n",
        "!pip install opencv-python==4.1.2.30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zvsi3F1MROQ"
      },
      "source": [
        "# Load model and config\n",
        "\n",
        "By default, we download our models from google drive. **However, in case that there are too many downloadings within a day, you can alternatively download model weights from onedrive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjonpOHZMwZm",
        "outputId": "04ae4e33-0ce0-4acc-8142-b82b24ceaf87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Text2Light/Text2Light/logs\n",
            "Retrieving folder contents\n",
            "Retrieving folder 1py2RGSofhP4-A3ucC_ISF6TJAqfuxhUO global_sampler_clip\n",
            "Retrieving folder 1EKcIuHEK59CNiKED_uhopZC8ViB3t3Np configs\n",
            "Processing file 1fuFkGFYlDz8PN9l9fgEY8-UYdsTEubhb 2022-04-23T05-49-02-lightning.yaml\n",
            "Processing file 1Vt5SybEdRLeMAcv5Vc2NbUIKxeHTiTi8 2022-04-23T05-49-02-project.yaml\n",
            "Retrieving folder 1azMVIl4RlmGYiAdtKQhrdPSmF4hcoB_0 checkpoints\n",
            "Processing file 17tBfIR9O1Bj7booNmmYisQTHqb3ErzvT last.ckpt\n",
            "Retrieving folder 1nb3IFyZsYMSenZhS1U30y4sV-6IZuuf8 local_sampler\n",
            "Retrieving folder 1J3SdXsb9dKA7SHGpViJTTA1ik8DBTpTK checkpoints\n",
            "Processing file 1ULUfZkLZBrLa7VNctnzo27qoGR15N6tm last.ckpt\n",
            "Retrieving folder 1CrnwAMcyKYCDcUyjpkppxnvzN_qqllfH configs\n",
            "Processing file 1hnO1v1R4uFddN9aFHhimi9u1ZjsTHYLR 2022-04-17T01-29-41-lightning.yaml\n",
            "Processing file 1xbfD8ay3JKyBhafDs9jF1ZFa205dJF9u 2022-04-17T01-29-41-project.yaml\n",
            "Retrieving folder 1kCdzDJIjDNRFi9SLoszCdtGouZKFqbM6 local_sampler_indoor\n",
            "Retrieving folder 1seo6NX6TEezFp98x7nA4GPOFRuuQIIJd checkpoints\n",
            "Processing file 1LtBXtP1j9xuMFP7HnNobv7te-KFw-mzc last.ckpt\n",
            "Retrieving folder 15nxDKzH3bbzrmIkShynv8XVE1CT3Nz1V configs\n",
            "Processing file 1-fFHpfbYkGBdUYvRsiix5TQ8hBLS_it6 2022-04-26T13-11-31-lightning.yaml\n",
            "Processing file 1f6DECTHj7Zv_8YMTiHpgvdA8otLUi3TD 2022-04-26T13-11-31-project.yaml\n",
            "Retrieving folder 1SPOK0GVqcyW2cmO7q8DnrSgTR9VGZz43 local_sampler_outdoor\n",
            "Retrieving folder 1MzjQtYMnt09FXeWcFgdYoPAaroc15wnC checkpoints\n",
            "Processing file 1d1IeVaP6hd7AL_lKBm_ncHXoyi-1A7zc last.ckpt\n",
            "Retrieving folder 1CBeUCsWkNdBEHIaFQdThERROz9-4ZDoE configs\n",
            "Processing file 10qkvog-f_xnZ1qLC1rLsNJsytaWBV5M- 2022-04-29T20-25-45-lightning.yaml\n",
            "Processing file 1VYbSDQmASJqDOjDTLI3wm0x1nJO30S_v 2022-04-29T20-25-45-project.yaml\n",
            "Processing file 1xDiuN0ETpWm7pt7D7VwttzZ5-It3s_0M inference_pldemo_ddim100_seed.zip\n",
            "Processing file 1FeUmgi5rNr9y_HCTUJBH28XcGozZN36U sritmo.pth\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fuFkGFYlDz8PN9l9fgEY8-UYdsTEubhb\n",
            "To: /content/Text2Light/Text2Light/logs/text2light_released_model/global_sampler_clip/configs/2022-04-23T05-49-02-lightning.yaml\n",
            "100% 77.0/77.0 [00:00<00:00, 239kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vt5SybEdRLeMAcv5Vc2NbUIKxeHTiTi8\n",
            "To: /content/Text2Light/Text2Light/logs/text2light_released_model/global_sampler_clip/configs/2022-04-23T05-49-02-project.yaml\n",
            "100% 1.55k/1.55k [00:00<00:00, 6.95MB/s]\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=17tBfIR9O1Bj7booNmmYisQTHqb3ErzvT\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "/content/Text2Light/Text2Light\n"
          ]
        }
      ],
      "source": [
        "%cd logs\n",
        "\n",
        "# using google drive\n",
        "!gdown --folder https://drive.google.com/drive/folders/1HKBjC7oQOzrkGFKMQmSh6PySv6AycDS3?usp=sharing\n",
        "\n",
        "# using onedrive, uncomment if download from onedrive\n",
        "# !mkdir -p text2light_released_model\n",
        "# !mkdir -p text2light_released_model/global_sampler_clip\n",
        "# !mkdir -p text2light_released_model/global_sampler_clip/checkpoints\n",
        "# !mkdir -p text2light_released_model/global_sampler_clip/configs\n",
        "# !mkdir -p text2light_released_model/local_sampler\n",
        "# !mkdir -p text2light_released_model/local_sampler/checkpoints\n",
        "# !mkdir -p text2light_released_model/local_sampler/configs\n",
        "# !mkdir -p text2light_released_model/local_sampler_indoor\n",
        "# !mkdir -p text2light_released_model/local_sampler_indoor/checkpoints\n",
        "# !mkdir -p text2light_released_model/local_sampler_indoor/configs\n",
        "# !mkdir -p text2light_released_model/local_sampler_outdoor\n",
        "# !mkdir -p text2light_released_model/local_sampler_outdoor/checkpoints\n",
        "# !mkdir -p text2light_released_model/local_sampler_outdoor/configs\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EUBiLI5EvFdBr_4fRFNeiScBflkVMSFyYwOqw-n5K8ziYA?e=by0I44&download=1\" -O text2light_released_model/global_sampler_clip/checkpoints/last.ckpt\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EYlVpiE_nJlGnGT48XrkOasBsyNhdZpo1o0kBv6GH9d3SQ?e=GnL9rZ&download=1\" -O text2light_released_model/global_sampler_clip/configs/2022-04-23T05-49-02-project.yaml\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/Ea4A8VTVNK9LsnGYpFdLMswB9l9vG1g5LGip3M7ZIIJh7Q?e=eeXqA7&download=1\" -O text2light_released_model/local_sampler/checkpoints/last.ckpt\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EaQ3yBgd4ehKpoowQHRiKRoBfG9UKetWgAv3KrbSAkyT1Q?e=UbhP4Y&download=1\" -O text2light_released_model/local_sampler/configs/2022-04-17T01-29-41-project.yaml\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EYPsy5KO71ZGphgk1YRaMAUB5EQ3GQVZwhF4QKmpASQLxg?e=fMvqU6&download=1\" -O text2light_released_model/sritmo.pth\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EceYDnnlH75GlDIpD87C6gsBq4nw98G2m1o8pzycQq23zw?e=4M30wk&download=1\" -O text2light_released_model/local_sampler_indoor/checkpoints/last.ckpt\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EWdN7JH-SN5EviwPu932v28BvX5B-4Pm10vzr0CVqRVHRA?e=3XTjD2&download=1\" -O text2light_released_model/local_sampler_indoor/configs/2022-04-26T13-11-31-project.yaml\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/EUP4I6v04PtDgBlNiMCQDtkB_LKnIT9zuQ2oFAWsoXiVkg?e=CewsEE&download=1\" -O text2light_released_model/local_sampler_outdoor/checkpoints/last.ckpt\n",
        "# !wget \"https://entuedu-my.sharepoint.com/:u:/g/personal/zhaoxi001_e_ntu_edu_sg/ERs_SkZ9f_VOoeYBk1xSC0MBkr3SHIs7rlykHKCkXxZhuA?e=hoAG4w&download=1\" -O text2light_released_model/local_sampler_outdoor/configs/2022-04-29T20-25-45-project.yaml\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_OWMFDIOtIQ"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hajNT6fSOsAq"
      },
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob\n",
        "import cv2\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import clip\n",
        "from taming.util import instantiate_from_config\n",
        "from sritmo.global_sritmo import SRiTMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Geb0BhsQiMw"
      },
      "source": [
        "# Define some utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WqRKG5FzQvyk"
      },
      "outputs": [],
      "source": [
        "def save_image(x, path):\n",
        "    c,h,w = x.shape\n",
        "    assert c==3\n",
        "    x = ((x.detach().cpu().numpy().transpose(1,2,0)+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "    s = Image.fromarray(x)\n",
        "    s.save(path)\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_knn(database: np.array, index: faiss.Index, txt_emb, k = 5):\n",
        "    dist, idx  = index.search(txt_emb, k)\n",
        "    return database[idx], idx #[bs, k, 512]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def text2light(models: dict, prompts, outdir, params: dict):\n",
        "    # models\n",
        "    global_sampler = models[\"gs\"]\n",
        "    local_sampler = models[\"ls\"]\n",
        "    # params\n",
        "    batch_size = len(prompts)\n",
        "    top_k = params[\"top_k\"]\n",
        "    temperature = params['temperature']\n",
        "    database = params['data4knn']\n",
        "    faiss_index = params['index4knn']\n",
        "    device = params['device']\n",
        "\n",
        "    # embed input texts\n",
        "    lan_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    lan_model.eval()\n",
        "    text = clip.tokenize(prompts).to(device)\n",
        "    text_features = lan_model.encode_text(text)\n",
        "    target_txt_emb = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    cond, _ = get_knn(database, faiss_index, target_txt_emb.cpu().numpy().astype('float32'))\n",
        "    txt_cond = torch.from_numpy(cond.reshape(batch_size, 5, cond.shape[-1]))\n",
        "    txt_cond = torch.cat([txt_cond, txt_cond,], dim=-1).to(device)\n",
        "\n",
        "    # sample holistic condition\n",
        "    bs = batch_size\n",
        "    start = 0\n",
        "    idx = torch.zeros(bs, 1, dtype=int)[:, :start].to(device)\n",
        "    cshape = [bs, 256, 8, 16]\n",
        "    sample = True\n",
        "\n",
        "    print(\"Generating holistic conditions according to texts...\")\n",
        "    for i in tqdm(range(start, cshape[2]*cshape[3])):\n",
        "        logits, _ = global_sampler.transformer(idx, embeddings=txt_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            logits = global_sampler.top_k_logits(logits, top_k)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        idx = torch.cat((idx, ix), dim=1)\n",
        "\n",
        "    xsample_holistic = global_sampler.decode_to_img(idx, cshape)\n",
        "    for i in range(xsample_holistic.shape[0]):\n",
        "        holistic_save = save_image(xsample_holistic[i], os.path.join(outdir, \"holistic\", \"holistic_[{}].png\".format(prompts[i])))\n",
        "\n",
        "    print(\"Synthesizing patches...\")\n",
        "    # synthesize patch by patch according to holistic condition\n",
        "    h = 512\n",
        "    w = 1024\n",
        "    xx, yy = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "    screen_points = np.stack([xx, yy], axis=-1)\n",
        "    coord = (screen_points * 2 - 1) * np.array([np.pi, np.pi/2])\n",
        "    spe = torch.from_numpy(coord).to(xsample_holistic).repeat(xsample_holistic.shape[0], 1, 1, 1).permute(0, 3, 1, 2)\n",
        "    spe = torch.nn.functional.interpolate(spe, scale_factor=1/8,\n",
        "                                            mode=\"bicubic\", recompute_scale_factor=False, align_corners=True)\n",
        "    spe = local_sampler.embedder(spe.permute(0, 2, 3, 1))\n",
        "    spe = spe.permute(0, 3, 1, 2)\n",
        "\n",
        "    _, h_indices = local_sampler.encode_to_h(xsample_holistic)\n",
        "    cshape = [xsample_holistic.shape[0], 256, h // 16, w // 16]\n",
        "    idx = torch.randint(0, 1024, (cshape[0], cshape[2], cshape[3])).to(h_indices)\n",
        "    idx = idx.reshape(cshape[0], cshape[2], cshape[3])\n",
        "\n",
        "    start = 0\n",
        "    start_i = start // cshape[3]\n",
        "    start_j = start % cshape[3]\n",
        "    sample = True\n",
        "\n",
        "    for i in tqdm(range(start_i, cshape[2])):\n",
        "        if i <= 8:\n",
        "            local_i = i\n",
        "        elif cshape[2]-i < 8:\n",
        "            local_i = 16-(cshape[2]-i)\n",
        "        else:\n",
        "            local_i = 8\n",
        "        for j in range(start_j, cshape[3]):\n",
        "            if j <= 8:\n",
        "                local_j = j\n",
        "            elif cshape[3]-j < 8:\n",
        "                local_j = 16-(cshape[3]-j)\n",
        "            else:\n",
        "                local_j = 8\n",
        "\n",
        "            i_start = i-local_i\n",
        "            i_end = i_start+16\n",
        "            j_start = j-local_j\n",
        "            j_end = j_start+16\n",
        "            patch = idx[:,i_start:i_end,j_start:j_end]\n",
        "            patch = patch.reshape(patch.shape[0],-1)\n",
        "            cpatch = spe[:, :, i_start*2:i_end*2,j_start*2:j_end*2]\n",
        "            cpatch = cpatch.reshape(cpatch.shape[0], local_sampler.cdim, -1)\n",
        "            patch = torch.cat((h_indices, patch), dim=1)\n",
        "            logits, _ = local_sampler.transformer(patch[:,:-1], embeddings=cpatch)\n",
        "            logits = logits[:, -256:, :]\n",
        "            logits = logits.reshape(cshape[0],16,16,-1)\n",
        "            logits = logits[:,local_i,local_j,:]\n",
        "            logits = logits / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                logits = local_sampler.top_k_logits(logits, top_k)\n",
        "            # apply softmax to convert to probabilities\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            # sample from the distribution or take the most likely\n",
        "            if sample:\n",
        "                ix = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "            idx[:,i,j] = ix.reshape(-1)\n",
        "    xsample = local_sampler.decode_to_img(idx, cshape)\n",
        "    for i in range(xsample.shape[0]):\n",
        "        ldr_save = save_image(xsample[i], os.path.join(outdir, \"ldr\", \"ldr_[{}].png\".format(prompts[i])))\n",
        "\n",
        "    # super-resolution inverse tone mapping\n",
        "    if params['sritmo'] is not None:\n",
        "        ldr_hr_samples, hdr_hr_samples = SRiTMO(xsample, params)\n",
        "    else:\n",
        "        print(\"no checkpoint provided, skip Stage II (SR-iTMO)...\")\n",
        "        return\n",
        "\n",
        "    for i in range(xsample.shape[0]):\n",
        "        ldr_hr_save = (ldr_hr_samples[i].permute(1, 2, 0).detach().cpu().numpy() + 1) * 127.5\n",
        "        cv2.imwrite(os.path.join(outdir, \"ldr\", \"hrldr_[{}].png\".format(prompts[i])), ldr_hr_save)\n",
        "        cv2.imwrite(os.path.join(outdir, \"hdr\", \"hdr_[{}].exr\".format(prompts[i])), hdr_hr_samples[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    return ldr_hr_save\n",
        "\n",
        "\n",
        "def load_model_from_config(config, sd, gpu=False, eval_mode=True):\n",
        "    if \"ckpt_path\" in config.params:\n",
        "        print(\"Deleting the restore-ckpt path from the config...\")\n",
        "        config.params.ckpt_path = None\n",
        "    if \"downsample_cond_size\" in config.params:\n",
        "        print(\"Deleting downsample-cond-size from the config and setting factor=0.5 instead...\")\n",
        "        config.params.downsample_cond_size = -1\n",
        "        config.params[\"downsample_cond_factor\"] = 0.5\n",
        "    try:\n",
        "        if \"ckpt_path\" in config.params.first_stage_config.params:\n",
        "            config.params.first_stage_config.params.ckpt_path = None\n",
        "            print(\"Deleting the first-stage restore-ckpt path from the config...\")\n",
        "        if \"ckpt_path\" in config.params.cond_stage_config.params:\n",
        "            config.params.cond_stage_config.params.ckpt_path = None\n",
        "            print(\"Deleting the cond-stage restore-ckpt path from the config...\")\n",
        "        if \"ckpt_path\" in config.params.holistic_config.params:\n",
        "            config.params.holistic_config.params.ckpt_path = None\n",
        "            print(\"Deleting the global sampler restore-ckpt path from the config...\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    model = instantiate_from_config(config)\n",
        "    if sd is not None:\n",
        "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "        print(f\"Missing Keys in State Dict: {missing}\")\n",
        "        print(f\"Unexpected Keys in State Dict: {unexpected}\")\n",
        "    if gpu:\n",
        "        model.cuda()\n",
        "    if eval_mode:\n",
        "        model.eval()\n",
        "    return {\"model\": model}\n",
        "\n",
        "def load_model(config, ckpt, gpu, eval_mode):\n",
        "    if ckpt:\n",
        "        raw_model = torch.load(ckpt, map_location=\"cpu\")\n",
        "        state_dict = raw_model[\"state_dict\"]\n",
        "    else:\n",
        "        raise NotImplementedError(\"checkpoint at [{}] is not found!\".format(ckpt))\n",
        "    model = load_model_from_config(config.model, state_dict, gpu=gpu, eval_mode=eval_mode)[\"model\"]\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ySYIWDtRAwi"
      },
      "source": [
        "# Settings for this run\n",
        "Mainly what you will have to modify will be `texts`. Please input your scene description in the text box which could be a free-form sentence. Given the limited GPU capacity of Colab, we only support generating one HDRI per inference time. To generate multiple HDRIs with a list of input texts, please refer to our [repo](https://github.com/FrozenBurning/Text2Light) for a full local installation.\n",
        "\n",
        "`model:` We use the model trained on our full dataset by default. Note that we also release models that trained on outdoor and indoor scenes respectively.\n",
        "\n",
        "`sritmo:` Specify sritmo to enable super-resolution and inverse tonemapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jk_HsuTmSFKb"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "texts = \"Outer space with distant stars, colorful nebulae, and multiple planets orbiting in the vast void, illuminated by two suns casting dynamic light across the scene\" #@param {type:\"string\"}\n",
        "model = \"full\" #@param [\"full\", \"outdoor\", \"indoor\"]\n",
        "sritmo = True #@param {type:\"boolean\"}\n",
        "sr_factor = 4 #@param {type:\"number\"}\n",
        "top_k = 100 #@param {type:\"number\"}\n",
        "temperature = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "local_sampler_path = None\n",
        "if model == \"full\":\n",
        "  local_sampler_path = \"./logs/text2light_released_model/local_sampler/\"\n",
        "elif model == \"outdoor\":\n",
        "  local_sampler_path = \"./logs/text2light_released_model/local_sampler_outdoor/\"\n",
        "elif model == \"indoor\":\n",
        "  local_sampler_path = \"./logs/text2light_released_model/local_sampler_indoor/\"\n",
        "else:\n",
        "  raise NotImplementedError\n",
        "\n",
        "sritmo_path = None\n",
        "if sritmo:\n",
        "  sritmo_path = \"./logs/text2light_released_model/sritmo.pth\"\n",
        "\n",
        "\n",
        "opt = argparse.Namespace(\n",
        "    resume_global=\"./logs/text2light_released_model/global_sampler_clip/\",\n",
        "    resume_local=local_sampler_path,\n",
        "    sritmo=sritmo_path,\n",
        "    sr_factor=sr_factor,\n",
        "    outdir=\"./text2light_generated\",\n",
        "    clip=\"./clip_emb.npy\",\n",
        "    text=texts,\n",
        "    top_k=top_k,\n",
        "    temperature=temperature,\n",
        "    bs=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7arlachIXBmq"
      },
      "source": [
        "# Run Text2Light!\n",
        "All outputs (including high-resolution HDR, LDR) will be saved in `./text2light_generated`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "l7h-ILKfXDzP",
        "outputId": "a54ba042-f960-45f0-c2af-feb5e667dd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from global sampler ckpt...\n",
            "logdir:./logs/text2light_released_model/global_sampler_clip\n",
            "./logs/text2light_released_model/global_sampler_clip/checkpoints/last.ckpt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './logs/text2light_released_model/global_sampler_clip/checkpoints/last.ckpt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-027de206138d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mglobal_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-303e08387053>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(config, ckpt, gpu, eval_mode)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mraw_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/text2light_released_model/global_sampler_clip/checkpoints/last.ckpt'"
          ]
        }
      ],
      "source": [
        "sys.path.append(os.getcwd())\n",
        "unknown=[]\n",
        "gpu = False\n",
        "eval_mode = True\n",
        "show_config = False\n",
        "\n",
        "base = list()\n",
        "\n",
        "ckpt = None\n",
        "if opt.resume_global:\n",
        "    if not os.path.exists(opt.resume_global):\n",
        "        raise ValueError(\"Cannot find {}\".format(opt.resume_global))\n",
        "    print(\"Resuming from global sampler ckpt...\")\n",
        "    assert os.path.isdir(opt.resume_global), opt.resume_global\n",
        "    logdir = opt.resume_global.rstrip(\"/\")\n",
        "    ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
        "    print(f\"logdir:{logdir}\")\n",
        "    base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*-project.yaml\")))\n",
        "    config2load = base_configs + base\n",
        "\n",
        "configs = [OmegaConf.load(cfg) for cfg in config2load]\n",
        "cli = OmegaConf.from_dotlist(unknown)\n",
        "config = OmegaConf.merge(*configs, cli)\n",
        "\n",
        "print(ckpt)\n",
        "if show_config:\n",
        "    print(OmegaConf.to_container(config))\n",
        "\n",
        "global_sampler = load_model(config, ckpt, gpu, eval_mode)\n",
        "\n",
        "ckpt = None\n",
        "if opt.resume_local:\n",
        "    if not os.path.exists(opt.resume_local):\n",
        "        raise ValueError(\"Cannot find {}\".format(opt.resume_local))\n",
        "    print(\"Resuming from local sampler ckpt...\")\n",
        "    assert os.path.isdir(opt.resume_local), opt.resume_local\n",
        "    logdir = opt.resume_local.rstrip(\"/\")\n",
        "    ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
        "    print(f\"logdir:{logdir}\")\n",
        "    base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*-project.yaml\")))\n",
        "    config2load = base_configs + base\n",
        "\n",
        "configs = [OmegaConf.load(cfg) for cfg in config2load]\n",
        "cli = OmegaConf.from_dotlist(unknown)\n",
        "config = OmegaConf.merge(*configs, cli)\n",
        "print(ckpt)\n",
        "if show_config:\n",
        "    print(OmegaConf.to_container(config))\n",
        "\n",
        "local_sampler = load_model(config, ckpt, gpu, eval_mode)\n",
        "\n",
        "outdir = opt.outdir\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "print(\"Writing samples to \", outdir)\n",
        "for k in [\"holistic\", \"ldr\", \"hdr\"]:\n",
        "    os.makedirs(os.path.join(outdir, k), exist_ok=True)\n",
        "\n",
        "prompts_file = opt.text\n",
        "if os.path.exists(prompts_file):\n",
        "    # list of prompts for text2light tasks\n",
        "    with open(prompts_file, 'r') as f:\n",
        "        prompts = f.read().splitlines()\n",
        "else:\n",
        "    # a single prompt\n",
        "    prompts = [prompts_file]\n",
        "\n",
        "# construct knn searching base\n",
        "if os.path.isfile(opt.clip):\n",
        "    clip_emb = np.load(opt.clip).astype('float32')\n",
        "else:\n",
        "    raise NotImplementedError('The path [{}] to clip embedding is not valid.'.format(opt.clip))\n",
        "\n",
        "knn_index = faiss.IndexFlatIP(clip_emb.shape[-1])\n",
        "knn_index.add(clip_emb)\n",
        "\n",
        "input_models = {\n",
        "    'gs': global_sampler,\n",
        "    'ls': local_sampler,\n",
        "}\n",
        "\n",
        "input_params = {\n",
        "    'top_k': opt.top_k,\n",
        "    'temperature': opt.temperature,\n",
        "    'device': 'cuda' if gpu else 'cpu',\n",
        "    'data4knn': clip_emb,\n",
        "    'index4knn': knn_index,\n",
        "    'sritmo': opt.sritmo,\n",
        "    'sr_factor': opt.sr_factor,\n",
        "}\n",
        "for i in range(0, len(prompts), opt.bs):\n",
        "    end_i = min(len(prompts), i + opt.bs)\n",
        "    prompt = prompts[i: i+opt.bs]\n",
        "    result = text2light(input_models, prompt, outdir, input_params)\n",
        "    display(Image.fromarray(result[:, :, [2, 1, 0]].clip(0,255).astype(np.uint8)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}